{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213efec7-8428-4131-a958-bb9660c7d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Random Forest Regressor?\n",
    "Ans.\n",
    "Random forest regressio is a type of Ensemble learning algorithm for regression tasks.It Consists of\n",
    "multiple decision trees. Each tree trained on a subset of data.The Output is the average (or median) \n",
    "of individual tree predictions. It is Reduces overfitting, handles non-linear relationships, and \n",
    "provides robust predictions for regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6922ba-4d30-4bbc-ae3b-e1c07b90b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
    "Ans.\n",
    "Random Forest Regressor reduces overfitting by Combines predictions from multiple trees. Trains each tree\n",
    "on a random subset of the data. It Considers different subsets of features for each split. And It Aggregates\n",
    "predictions, smoothing out individual trees overfitting tendencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fabf9a-0805-4138-b0bd-871e57ae4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
    "Ans.\n",
    "The Random Forest Regressor aggregates predictions by averaging the outputs of multiple decision trees. Each\n",
    "tree in the ensemble independently predicts the target variable, and the final prediction for a given input\n",
    "is the average (or mean) of these individual tree predictions. This process helps to reduce the impact of \n",
    "individual tree variations, noise, and overfitting tendencies, resulting in a more robust and generalized\n",
    "regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924a35d-9440-460a-9da6-2e4435b9612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are the hyperparameters of Random Forest Regressor?\n",
    "Ans.\n",
    "Hyperparameters of Random Forest Regressor:\n",
    "1. n_estimators: Number of trees in the forest.\n",
    "2. max_depth: Maximum depth of each tree.\n",
    "3. min_samples_split: Minimum number of samples required to split an internal node.\n",
    "4. min_samples_leaf: Minimum number of samples required to be at a leaf node.\n",
    "5. max_features: Maximum number of features considered for splitting a node.\n",
    "6. bootstrap: Whether to use bootstrap samples.\n",
    "7. random_state: Seed for random number generation.\n",
    "8. n_jobs: Number of jobs to run in parallel during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125eba5a-1f37-4212-bf44-e0f36e321ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
    "Ans.\n",
    "The Random Forest Regressor and Decision Tree Regressor differ primarily in their approach to handling \n",
    "overfitting and making predictions. The Random Forest Regressor is an ensemble method that builds multiple\n",
    "decision trees, introducing randomness in both data sampling and feature selection. It reduces overfitting\n",
    "by averaging the predictions of these trees, providing a more robust and generalized regression model. In\n",
    "contrast, the Decision Tree Regressor constructs a single tree and is more susceptible to overfitting, \n",
    "capturing intricate details in the training data. While simpler, the Decision Tree Regressor may lack the\n",
    "generalization capabilities and stability offered by the ensemble nature of the Random Forest Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3afa7b-8b10-44a8-aada-d8da6cf031b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
    "Ans.\n",
    "Advantages of Random Forest Regressor:\n",
    "1. Robustness: Resistant to overfitting due to ensemble averaging.\n",
    "2. High Accuracy: Generally provides accurate predictions.\n",
    "3. Feature Importance: Can rank features by their contribution to the model.\n",
    "4. Handles Non-linearity: Effective for capturing complex relationships in data.\n",
    "\n",
    "Disadvantages of Random Forest Regressor:\n",
    "1. Complexity: Ensemble of trees can be computationally intensive and complex.\n",
    "2. Lack of Interpretability: Interpretability is reduced compared to a single decision tree.\n",
    "3. Memory Usage: Requires more memory than a single decision tree.\n",
    "4. Black Box Nature: The ensembles predictions may be seen as a \"black box\" without clear insights into \n",
    "individual trees decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ebd35-6b98-4d07-a19c-f9397c6e9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. What is the output of Random Forest Regressor?\n",
    "Ans.\n",
    "The output of a Random Forest Regressor is a continuous numerical value. For each input, the ensemble of \n",
    "decision trees in the Random Forest independently predicts a numerical output, and the final prediction is\n",
    "often the average (or mean) of these individual tree predictions. This aggregated output represents the \n",
    "Random Forest Regressors prediction for the given input in a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3edfd8d-2f38-4c03-88ab-b23751f79f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Can Random Forest Regressor be used for classification tasks?\n",
    "Ans.\n",
    "the Random Forest Regressor can be adapted for classification tasks. Despite its name, it can handle both \n",
    "regression and classification. In classification, it assigns class labels based on the majority vote or \n",
    "probability from the ensemble of decision trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
